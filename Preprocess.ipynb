{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPRwRI1nADUZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import utils\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline\n",
        "from collections import defaultdict\n",
        "from tokenizers.decoders import WordPiece"
      ],
      "metadata": {
        "id": "HIad1CjyAtTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessing:\n",
        "    \"\"\"\n",
        "    A class for preprocessing parliamentary conversation data, specifically designed for Knesset protocols.\n",
        "\n",
        "    This class handles various preprocessing tasks including:\n",
        "    - Loading and saving data from specified paths\n",
        "    - Unifying committee names using Levenshtein distance\n",
        "    - Splitting long conversations into smaller, manageable segments\n",
        "    - Cleaning and structuring conversation data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_path, save_path, base_path=\"\", filter_data=True, create_new=False):\n",
        "        \"\"\"\n",
        "        Initialize the Preprocessing class with specified paths and settings.\n",
        "        \"\"\"\n",
        "        self.data_path = os.path.join(base_path, data_path)\n",
        "        self.save_path = os.path.join(base_path, save_path)\n",
        "\n",
        "        # if save path already exists, don't create new data\n",
        "        if os.path.exists(self.save_path) and not create_new:\n",
        "            self.data_path = self.save_path\n",
        "            self.data = utils.load_data(data_path)\n",
        "            return\n",
        "\n",
        "        self.data = utils.load_data(data_path)\n",
        "        self.data = self.data[['committee_name', 'session_id', 'chairperson', 'speaker_name', 'conversation']]\n",
        "        self.data.dropna(inplace=True)\n",
        "        self.unify_committees()\n",
        "        self.accurate_conv()\n",
        "        self.data.sort_values(by=['session_id'], inplace=True)\n",
        "        utils.save_data(self.data, self.save_path)\n",
        "\n",
        "    def unify_committees(self):\n",
        "        \"\"\"\n",
        "        Unify similar committee names using Levenshtein distance.\n",
        "\n",
        "        This method identifies and groups similar committee names to standardize\n",
        "        committee naming across the dataset. It uses Levenshtein distance to\n",
        "        measure string similarity and groups committees with distance < 0.3.\n",
        "\n",
        "        The method modifies the 'committee_name' column in self.data directly.\n",
        "        \"\"\"\n",
        "        committees = self.data['committee_name'].unique()\n",
        "        committees = [c for c in committees if 'ועד' in c]\n",
        "\n",
        "        committee_groups = defaultdict(list)\n",
        "        for c in committees:\n",
        "            curr_group = []\n",
        "            for c2 in committees:\n",
        "                if c == c2:\n",
        "                    continue\n",
        "                levenshtein_score = utils.normalized_levenshtein(c, c2)\n",
        "                if levenshtein_score < 0.3:\n",
        "                    curr_group.append((c2, levenshtein_score))\n",
        "            committee_groups[c] = curr_group\n",
        "\n",
        "        committee_mapping = {}\n",
        "        for name, similar_names in tqdm(committee_groups.items()):\n",
        "            if name not in committee_mapping:\n",
        "                chosen_name = name\n",
        "            else:\n",
        "                chosen_name = committee_mapping[name][0]\n",
        "\n",
        "            for similar_name, score in similar_names:\n",
        "                if similar_name not in committee_mapping:\n",
        "                    committee_mapping[similar_name] = (chosen_name, score)\n",
        "                else:\n",
        "                    if committee_mapping[similar_name][1] > score:\n",
        "                        committee_mapping[similar_name] = (chosen_name, score)\n",
        "\n",
        "        self.data['committee_name'] = self.data['committee_name'].apply(\n",
        "            lambda x: committee_mapping[x][0] if x in committee_mapping else x\n",
        "        )\n",
        "\n",
        "    def accurate_conv(self):\n",
        "        \"\"\"\n",
        "        Split long conversations into smaller, speaker-specific segments.\n",
        "\n",
        "        This method processes conversations that are longer than 40 words by:\n",
        "        1. Using DICTA-BERT NER model to identify speakers in the text\n",
        "        2. Splitting the conversation at speaker transitions\n",
        "        3. Creating new rows in the dataset for each speaker segment\n",
        "        \"\"\"\n",
        "        oracle = pipeline('ner', model='dicta-il/dictabert-ner', aggregation_strategy='simple')\n",
        "        oracle.tokenizer.backend_tokenizer.decoder = WordPiece()\n",
        "        remove_from_df = []\n",
        "        add_to_df = {}\n",
        "        for inx, conv in enumerate(self.data['conversation']):\n",
        "            if len(conv.split()) > 40:\n",
        "                entities = oracle(conv)\n",
        "                speakers = [speaker for speaker in entities if speaker['entity_group'] == 'PER']\n",
        "                if len(speakers) == 0:\n",
        "                    continue\n",
        "                remove_from_df.append(inx)\n",
        "                prev = speakers.pop(0)\n",
        "                if len(speakers) == 0:\n",
        "                    if add_to_df.get(inx) is None:\n",
        "                        add_to_df[inx] = []\n",
        "                    add_to_df[inx].append((prev['word'], conv))\n",
        "                for speaker in speakers:\n",
        "                    speaker_turn = conv[prev['end']:speaker['start']]\n",
        "                    speaker_turn = re.sub(r'\\b(?:יו\"ר|יור|היו\"ר)\\b', '', speaker_turn)\n",
        "                    if add_to_df.get(inx) is None:\n",
        "                        add_to_df[inx] = []\n",
        "                    add_to_df[inx].append((prev['word'], speaker_turn))\n",
        "                    prev = speaker\n",
        "            print(f\"Done {inx}/{len(self.data)}\", end='\\r')\n",
        "        print(\"\\n\")\n",
        "        i = 0\n",
        "        for inx in remove_from_df:\n",
        "            print(f\"Replace {inx}/{len(remove_from_df)}\", end='\\r')\n",
        "            row = self.data.iloc[inx - i]\n",
        "            self.data.drop(self.data.index[inx - i], inplace=True)\n",
        "            i += 1\n",
        "            for speaker, turn in add_to_df[inx]:\n",
        "                new_index = self.data.index.max() + 1\n",
        "                new_row = {\n",
        "                    'committee_name': row[0],\n",
        "                    'session_id': row[1],\n",
        "                    'chairperson': row[2],\n",
        "                    'speaker_name': speaker,\n",
        "                    'conversation': turn\n",
        "                }\n",
        "                self.data.loc[new_index] = new_row"
      ],
      "metadata": {
        "id": "iAf6hc5uASet"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}