{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a18b8e25-531c-44d4-9333-e0a7e2475b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "310c987d-3b7a-46ab-a427-7a8249607d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffensiveWordsDetectorWithContext:\n",
    "    def __init__(self):\n",
    "        # רשימת המילים הפוגעניות\n",
    "        self.offensive_words = [\n",
    "        \"אהצבועה\", \"אובססיבית\", \"איכס\", \"אינעל\",\"שמוק\",\"דביל\",\"בולבול\",\"קפוץ לי\", \"אנטיפתי\", \"אסתור\", \"ב\\\"ביצים\", \"בביצים\", \"בגועליציה\", \"בגזענותו\",\n",
    "        \"בוזין\", \"בזוי\", \"בזויים\", \"בחרא\", \"ביריונים\", \"בנזונה\", \"בקקה\", \"בשמוק\", \"בתחת\", \"בתחתונים\", \"גונב\", \"גונבי\",\n",
    "        \"גנב\", \"גנבים\", \"גנבת\", \"דימגוג\", \"דפוק\", \"דפוקים\", \"דרקים\", \"הארכיטיפ\", \"הבוגדים\", \"הזבל\", \"הזבלל\", \"הזדוניים\",\n",
    "        \"הזוי\", \"הזויה\", \"הזויים\", \"החאפרים\", \"החארות\", \"החולנית\", \"החולרה\", \"החוצפן\", \"החמור\", \"יחמור\", \"החפרן\",\n",
    "        \"היללן\", \"המבהיל\", \"המבחילה\", \"המושרצים\", \"המזורגג\", \"המחורבן\", \"המחורבנות\", \"המחורבנים\", \"המחורבנת\", \"המטומטם\",\n",
    "        \"המטומטמת\", \"המנוולת\", \"המניאקים\", \"המנייאק\", \"המסריח\", \"המסריחים\", \"המפלצת\", \"המתוסבכתנכדתו\", \"הסטן\",\n",
    "        \"הסרחוני\", \"הפאקינג\", \"הפושע\", \"הפושעים\", \"הפראייר\", \"הציבוז'י\", \"הקקה\", \"הרשע\", \"הרשעים\", \"הרשעית\", \"השמוק\",\n",
    "        \"השמוקים\", \"השקרן\", \"השקרנית\", \"השרלטנות\", \"התחת\", \"וגנב\", \"ודרעק\", \"והנבער\", \"והנצלנית\", \"והשפל\", \"וחלאות\",\n",
    "        \"וחרא\", \"וכוסאמק\", \"ולברבר\", \"ולעלובי\", \"ומנוול\", \"ומנוולים\", \"ומסריח\", \"וסורר\", \"וסטנים\", \"וסמרטוטית\",\n",
    "        \"וערמומית\", \"ופחדן\", \"ופטפטני\", \"ופסיכופטים\", \"וצבוע\", \"וצבועה\", \"וציפורע\", \"וקשקשן\",\"קשקשן\" \"ורמאי\", \"ורמאים\",\n",
    "        \"ושקרן\", \"ושקרנים\", \"ושקרנית\", \"זבל\", \"זבלה\", \"זבלים\", \"זומבים\", \"חאלס\", \"חאפר\", \"חאפרים\", \"חאפרית\",\n",
    "        \"חובבן\", \"חולני\", \"חזיר\", \"חירבנה\", \"חמוריםםם\", \"חרא\", \"חראא\", \"חראאא\", \"טינופת\", \"טמטום\", \"יאוכלי\", \"יבן\",\n",
    "        \"יוק\", \"יזבל\", \"יזבלה\", \"יזבל\", \"יחתיכת\", \"יטמבל\", \"יליצן\", \"ימגעיל\", \"ימיץ\", \"ימניאק\", \"ימניייק\", \"ימנייק\",\n",
    "        \"ימניק\", \"ימעפן\", \"ינעל\", \"כאןצבועה\", \"ככלבלב\", \"יכלב\", \"כנפיחה\", \"כפודל\", \"כשחלאה\", \"לאזעזל\", \"להזויים\",\n",
    "        \"להשתין\", \"לזרגג\", \"לחאפר\", \"לחרא\", \"לחרבן\", \"לךגזען\", \"למותי\", \"לעזאזאל\", \"לעזזאל\", \"לקק\", \"לשקרנים\",\n",
    "        \"מאוסה\", \"מאפן\", \"מבהיל\", \"מבהילה\", \"מבהילים\", \"מבחילה\", \"מגעיל\", \"מגעילה\", \"מגעילים\", \"מהאדיוט\", \"מזוויעה\",\n",
    "        \"מזוויעים\", \"מזורגג\", \"מזנים\", \"מחורבן\", \"מחורבנות\", \"מחורבנת\", \"מטונף\", \"מלהתקרצץ\", \"מלשין\", \"מלשינה\",\n",
    "        \"ממזרים\", \"מנובל\", \"מנובלים\", \"מניאק\", \"מנייאק\", \"מנייק\", \"מניפולטורית\", \"מניפולטיבית\", \"מניק\", \"מנניאיק\",\n",
    "        \"מסקן\", \"מסקנים\", \"מסריח\", \"מסריחה\", \"מסריחות\", \"מסריחים\", \"מעפן\", \"מעפנהה\", \"מפגר\", \"מפגרים\", \"מפגרת\",\n",
    "        \"מפחידה\", \"משקר\", \"משקרים\", \"משתין\", \"משתינה\", \"משתינות\", \"משתינים\", \"נבזי\", \"נבזים\", \"נוכל\", \"נוכלות\",\n",
    "        \"נוכלים\", \"נוכלת\", \"נחש\", \"ניבזה\", \"נצלנים\", \"סאדיסטי\", \"סטן\", \"סטנים\", \"סטרפלצט\", \"סמרטוט\", \"עוכר\",\n",
    "        \"עוכרי\", \"עוכרת\", \"פָּתט\", \"פאקינג\", \"פברק\", \"פברקו\", \"פושע\", \"פושעת\", \"יפח\", \"פחדנים\", \"פחזבל\", \"פיפי\",\n",
    "        \"פלוץ\", \"פתטי\", \"פתי\", \"צבוע\", \"צבועה\", \"צבועים\", \"קברן\", \"קברניות\", \"קברנים\", \"קברנית\", \"קיבינמט\",\n",
    "        \"קקאיות\", \"קקה\", \"קקות\", \"קקי\", \"קרימינל\", \"קרימינלית\", \"רבאק\", \"רמאי\", \"רמאים\", \"רמאית\", \"רשלן\",\n",
    "        \"שאובססיבית\", \"שהמפלצת\", \"שודד\", \"שונא\", \"שונאת\", \"שטחית\", \"שלוזרים\", \"שמוק\", \"שמלאני\", \"שמלאנים\",\n",
    "        \"שמלנים\", \"שפיצרקה\", \"שקלוזה\", \"שקרן\", \"שקרני\", \"שקרניות\", \"שקרנים\", \"שקרנית\", \"שרלטן\", \"שרלטנים\",\n",
    "        \"שרלטנית\", \"שתחנק\", \"שתמות\", \"שתמותי\", \"שתסתמי\", \"שתשרפי\", \"תחמן\", \"תחמניות\", \"תחמנים\", \"תחמנית\",\n",
    "        \"תיסתום\", \"תישארמה\", \"תמותי\", \"תנענע\", \"תסתום\", \"תסתמו\", \"תסתמי\", \"תעופי\", \"תעפו\", \"תשרף\", \"תשתכשכי\"\n",
    "        ]\n",
    "        \n",
    "        # ביטויים של קריאה לסדר\n",
    "        self.order_calls = [\n",
    "            'אני קורא אותך לסדר',\n",
    "            'אני קוראת אותך לסדר',\n",
    "            'זאת אזהרה אחרונה',\n",
    "            'קריאה לסדר'\n",
    "        ]\n",
    "        \n",
    "        # סימוני הפרעה\n",
    "        self.interruption_marks = [\"- - -\", \"- -\"]\n",
    "    def analyze_text(self, text):\n",
    "        \"\"\"Analyze a single text for all types of content\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return {\n",
    "                'offensive_found': 0,\n",
    "                'order_calls_found': 0,\n",
    "                'interruption_found': 0,\n",
    "                'found_words': ''\n",
    "            }\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        found_words = []\n",
    "        \n",
    "        # Check for offensive words\n",
    "        offensive_found = 0\n",
    "        for word in self.offensive_words:\n",
    "            if word in text_lower:\n",
    "                offensive_found = 1\n",
    "                found_words.append(word)\n",
    "        \n",
    "        # Check for order calls\n",
    "        order_calls_found = 0\n",
    "        for call in self.order_calls:\n",
    "            if call in text:\n",
    "                order_calls_found = 1\n",
    "                found_words.append(call)\n",
    "        \n",
    "        # Check for interruptions\n",
    "        interruption_found = 0\n",
    "        for mark in self.interruption_marks:\n",
    "            if mark in text:\n",
    "                interruption_found = 1\n",
    "                found_words.append(mark)\n",
    "        \n",
    "        return {\n",
    "            'offensive_found': offensive_found,\n",
    "            'order_calls_found': order_calls_found,\n",
    "            'interruption_found': interruption_found,\n",
    "            'found_words': ', '.join(found_words) if found_words else ''\n",
    "        }\n",
    "\n",
    "    def process_batch(self, batch_num):\n",
    "        \"\"\"Process a single batch file\"\"\"\n",
    "        try:\n",
    "            file_path = f\"/home/gorelikk/NLP-PROJECT/Pre_Process/Pre_Process/batch_{batch_num}_processed_data.csv\"\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"\\nProcessing batch {batch_num}, {len(df)} rows\")\n",
    "            \n",
    "            # Initialize lists for columns\n",
    "            offensive_flags = []\n",
    "            order_call_flags = []\n",
    "            interruption_flags = []\n",
    "            found_words_list = []\n",
    "            \n",
    "            # Process each row\n",
    "            for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "                analysis = self.analyze_text(row['conversation'])\n",
    "                offensive_flags.append(analysis['offensive_found'])\n",
    "                order_call_flags.append(analysis['order_calls_found'])\n",
    "                interruption_flags.append(analysis['interruption_found'])\n",
    "                found_words_list.append(analysis['found_words'])\n",
    "            \n",
    "            # Add new columns to the dataframe\n",
    "            df['offensive_words_found'] = offensive_flags\n",
    "            df['order_calls_found'] = order_call_flags\n",
    "            df['interruption_found'] = interruption_flags\n",
    "            df['found_words'] = found_words_list\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {batch_num}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_all_batches(self):\n",
    "        \"\"\"Process all batch files\"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        for batch_num in range(1, 16):\n",
    "            batch_df = self.process_batch(batch_num)\n",
    "            if batch_df is not None:\n",
    "                all_results.append(batch_df)\n",
    "        \n",
    "        if all_results:\n",
    "            # Combine all results\n",
    "            final_df = pd.concat(all_results, ignore_index=True)\n",
    "            \n",
    "            # Save the results\n",
    "            final_df.to_csv(\"Detect/all_batches_analysis.csv\", index=False)\n",
    "            print(f\"\\nResults saved to: all_batches_analysis.csv\")\n",
    "            \n",
    "            # Print summary statistics\n",
    "            total_rows = len(final_df)\n",
    "            offensive_count = sum(final_df['offensive_words_found'])\n",
    "            order_calls_count = sum(final_df['order_calls_found'])\n",
    "            interruption_count = sum(final_df['interruption_found'])\n",
    "            \n",
    "            print(\"\\n=== Summary ===\")\n",
    "            print(f\"Total conversations analyzed: {total_rows:,}\")\n",
    "            print(f\"Conversations with offensive words: {offensive_count:,} ({offensive_count/total_rows*100:.2f}%)\")\n",
    "            print(f\"Conversations with order calls: {order_calls_count:,} ({order_calls_count/total_rows*100:.2f}%)\")\n",
    "            print(f\"Conversations with interruptions: {interruption_count:,} ({interruption_count/total_rows*100:.2f}%)\")\n",
    "            \n",
    "            return final_df\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2b165-3519-4c8e-82fe-fd1cb874a70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 1, 509958 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509958/509958 [00:37<00:00, 13452.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 2, 538803 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538803/538803 [00:39<00:00, 13783.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 3, 480954 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 480954/480954 [00:35<00:00, 13524.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 4, 581672 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 581672/581672 [00:41<00:00, 14092.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 5, 629831 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 629831/629831 [00:43<00:00, 14336.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 6, 504450 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 504450/504450 [00:34<00:00, 14559.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 7, 284263 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284263/284263 [00:17<00:00, 15913.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 8, 556595 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 556595/556595 [00:36<00:00, 15351.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 9, 961182 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961182/961182 [01:01<00:00, 15711.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 10, 688563 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 295858/688563 [00:19<00:26, 15020.42it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    detector = OffensiveWordsDetectorWithContext()\n",
    "    results = detector.process_all_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed68623-3bdf-468d-94d0-7e7cce3c6412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "setfit_env",
   "language": "python",
   "name": "setfit_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
